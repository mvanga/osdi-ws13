\section{Multi/Many-Core Architectures}
\label{sec:overview}
We distinguish the notions of multi- and many-core
architectures acording to the following definition~\cite{Re:12}:
\emph{multi-core} systems can be thought of as the iterative squeezing of powerful
 single-core chips that
were developed till around 2005 onto a single chip. Examples include the
 Intel Sandy Bridge line of processors which directly use previously developed chips
and add additional functionality needed for supporting multiple cores. This
contrasts with \emph{many-core} architectures, which are built from the ground up 
to support massive parallelism at the cost of slower processor speeds,
with a large number of fairly simple cores.  Examples include
the new Intel Xeon processors and the Tilera~\cite{bell2008tile64} line of massively parallel proecssors.
These represent a significant paradigm shift over previous designs.

We highlight various architectural designs present in commercial off-the-shelf
(\emph{COTS}) hardware. These represent particular points in a large design
spaces.
\begin{itemize}
\item {\bf Number of Cores - Parallelism} Different architectures effect a tradeoff between the
number of cores and the simplicity of each core. Designs with fewer,
more complex cores optimise for serial performance (Intel Sandy Bridge)
whereas large number of simple cores are explicitly geared towards
optimising thread-level and data-level parallelism.
\item {\bf Heterogeineity} In seeking to strike a balance between parallel and
serial performance, existing processor designs combine multiple CPU cores with
different physical characteristics, optimised for different tasks. The many-core Cell BE ~\cite{Cell} thus combines 2 distict kinds of processors with different ISAs, by
contrast Tilera only considers identical cores.
\item {\bf Interconnection Links} The large majority of uniprocessor systems
used a ring-based interconnect. This is often no longer appropriate for
multi-/many-core systems due to interconnects having often become a bottleneck,
and the prohibitive delay induced by wires. New interconnect designs are being researched,
such as \emph{Intel QuickPath Interconnect} (QPI) and  AMD's HyperTransport technology.
Many modern systems are opting for ring-based, mesh and crossbar topologies.
\item {\bf Memory} The large majority of multi-/many-core systems prove
non-uniform access time to memory, distinguishing between local and remote
memory [CITE]. The memory hierarchy is increasingly complex.
Furthermore, synchronisation and maintaining strong memory consistency across
cors and layers is a significant challenge. As it stands, most modern
design continue to share memory, though there has been a recent push
towards shared-nothing software architecture. [CITE]
\end{itemize}
