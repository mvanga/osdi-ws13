\section{Challenges for Software}
\label{sec:challenges}

Software for multi- and many-core architectures faces many challeneges.
NATACHA: what do you mean by software here? applications or OS? Should we not
clearly separate out the two?

\subsection{Efficient Allocation of Resources}

- Scheduling seems simpler given a large enough number of cores and less than
full utilization of system resources.
- Can cores be allocated to applications? Space-wise scaling of the OS. What
about optimizing for power consumption or fairness?
- symmetric scheduling  or asymmetric scheduling?
- if opt for a design with very high number of very simple cores, how can we
guarantee good serial performance (Ahmdahl's law)

\subsection{Non-Uniformity}
- Non-uniformity in terms of memory accesses
- Non-uniformity in terms of interconnects and therefore
in associated time to communicate with appropriate devices

\subsection{Heterogeneity}

- ISA level, performance level and memory level
- How does software deal with this heterogeneity?
- How can we model and represent that heterogeneity efficiently?
- How much should we expose and at what level?
- Do we gain anything from this or is it something we must "cope with"?

\subsection{OS Abstractions}

- Are current OS abstractions still valid for these architectures?
- Do we need to rethink anything? For example, considering a hierarchical memory
space based on access latencies? Extend or get rid of the concept of processes?
- Number of OSes investigate this Barrelfish, Helios, Tesselation, Corey, fOs. What
are the common trends, where do they fall short?

\subsection{Applications}
- How should we design, as programmers, applications for multi/many core systems.
- Can't expect good serial performance. How do we change? 

\paragraph{Summary}
The key problem seems to be in understanding the tradeoffs that such architectures
provide. Also need to consider the new bottlenecks in these designs.


Other points that I can think of:
- key issue that arises in multicore architecture
is the sharing of resources which wasn't there previously
That can come up in the form of sharing global datastructures
(page tables, etc.), interference (TLB shootdown, cache pollution)
to physical resources (interconnect bottlenecks).
- Make the claim that whereas previously memory latency was the
problem, memory bandwidth is now a bigger issue (The ASPLOS
paper makes that point well)
- Amdhal's law states that any parallel speedup is limited by
the serial performance of the application, how do we guarantee
good serial performance if we only have very simple cores.
- Applications. (this is linked to abstractions, maybe?) How do
we design applications which are highly parallel, sufficiently
so that they can be run on a large number of cores efficiently,
or alternatively, what techniques can we use to
implicitly parallelise such applications to be able to make them
run on multiple cores (speculation comes in to play here)