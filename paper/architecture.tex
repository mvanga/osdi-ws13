
\section{Multicore Architectures}
This section surveys the major architectural decisions of modern multicore 
architectures. It identifies three broad categories:
architectures with a small number of powerful cores, designs with large number of simple, homogeneous cores, and
finally, processors which explicitly associate
cores of varying capabilities and instructions sets.
We compare and contrast the motivation behind these
design decisions and their impact on the competing
concerns of power, performance, programmability.
We observe that these reflect the diversification of computing
amonst different application domains. Processors for mobile devices
optimise for performance, reactivty and energy. Processors for desktop, 
optimise for cost-performance and  graphics. By contrast,
servers prioritise throughput, scaiability and energy, whilst
embedded devices optimise for energy, and application-specific
performance. 

\subsection{Architecture Overview}

We describe each category in turn, focusing on the main 
considerations which motivated the design, and illustrate
with a specific example.

\paragraph{Small number of complex high performance cores}
Various architectures place a small number (2-8) of highly
powerful cores on a chip.  Cores are homogeneous, fully cache-coherent,
and generally own a private cache, but share the last level cache. 
Each core aggressively exploits instruction level parallelism, through
out-of-order execution, speculation, very deep pipelines, etc. 
These designs first and foremost optimise sequential performance, 
and leverage instruction level parallelism over thread level parallelism. 
This comes at the expense of high power consumption, 
though most designs incorporate some power
saving techniques, including but not limited to, dynamic 
voltage scaling, processing gating, etc.
\paragraph{} The i7 Intel Core 920 for example contains 4 cores which aggresively
exploit ILP through a four issue,  dynamically scheduled 16 stage pipeline,
and multilevel branch prediction unit. All cores own
 private L1 and L2 caches, and share the L3 cache. They communicate
through a ring based interconnect. These architectures are
optimised for desktop style applications, where many applications continue
to be designed with uniprocessors in mind, and are therefore mostly sequential.
Ahmdahl's law states that the speedup optained by increasing parallelism 
is bounded by the sequential part of the application. Powerful processors are 
thus required to maintain good performance for these applications. 
Adding more cores simply allows for new optimisations to be explored, namely, 
thread level parallelism, over the diminishing returns obtained by trying
to further exploit ILP. 


\paragraph{Large Number of Simple, Low Frequency Cores} An alternative
design strategy consists in placing a larger number of simpler, lower frequency cores
onto the chip. Simpler cores consume significantly less power as
a result of the combined effect of 1) less static power consumption because 
of fewer components on the chip 2) the ability to scale voltage down as
the clock rate is smaller. ADD OTHER REASON. The Tilera
architecture integrates a large number of low power, low frequency 
cores (. Each processor is a compile-time scheduled three way VLIW processors.
Compile time scheduling of VLIW operations results in significantly
lower power usage than a dynamically scheduled superscalar
processor like the i7. Cores are homogeneous and share 
an instruction set to make it easier for applications to run on
this architecture.  The interconnect is a point to point mesh network, which
is highly scalable and minimises wire delays between cores. This
contrasts with the traditional ring based architecture, which,
through memory coherency protocols, quickly becomes a bottleneck
as the number of cores grows. Each
core has a private cache. All share a fully coherent dynamic
distributed L3 cache.  This design allows for high performance
if the workload is highly parallelisable. This is the case for many 
server workloads which optimise for throughput, and who require
the ability to scale whilst satisfying a low power budget.  

\paragraph{Heterogeneous Cores} The first category optimised for 
sequential performance and programmability at the expense of power. The second, 
for throughput, power and programmability at the expense of sequential performance. 
Core homogeneity, both in terms of instruction set and capability,
was key to facilitate application development by users. The last category
of multicore processors, by contrast, explicitly optimises both
power and performance, at the expense of programmibility. They do so in two ways: 
by associating cores with different capabilities on a chip, and possibly components
with different ISAs. This allows them to run tasks on the core
that is best suited for that workload: a mostly sequential taks will be run 
on a highly powerful core, whilst an easliy parraleisable workload
will be run on multiple simple cores. Similarly, certain chips incorporate
application-specific devices, such as GPUs or FPGAs which can achieve extremely
high performance in specific cases. Appropriately scheduling each 
task on the appropriate core complicates the task of the OS scheduler.
Programming for multiple ISAs increases the complexity of a developer's code. 

\paragraph{}The Cell BE illustrates the benefits obtainable with heterogeneous
cores with distinct ISAs. The processor combines a 
dual issue general power architecture compliant core (Power Processing Element), and 8 ``coprocessors''
cores (Synergistic Processing Elements). These implement a new instruction sete
architecture optmised for power and performance.  The PPE has control over the SPE
and decides of task allocations, which processes to run, etc. SPEs can only 
access their own local memory, and rely on DMA for all other forms of computation.
The system impleents a fully cache coherent DMA to access both memory and disk. 
The PPE and all the SPEs communicate via a high bandwdith bus. 
Programming on the Cell presents two challenges: firstly, the different kinds of 
instruction sets present. And secondly, the existence of local store memory and the
fact that software must manage that memory. The choice of having two different instruction
sets can be explained by the desire to be able to run a standard OS one the main 
core whilst uploading computationally intensive/highly parallel work 
to the SPEs. Having a specialised architecture set optimised for power was
because of the stated objective: icnrease a large icnrease in performance,
keeping the pwoer budget small.  It's worth to note though that the 
fully coherent DMA does make things easier.  Cell BE is used in 
many game consoles. A similar design decision was taken in the Nvida Tigra 2, which is 
a processor geared towards mobile systems. It integrates a dual
core ARM Cortex A9 and GPU on a single chip using a single physical memory. 

\paragraph{} The BigLittle processor struck a different tradeoff,
and expressly chose to have cores of different capabilities, but
with the same instruction set. The Arm LittleBig is a processor
deisgned with the goal of improving power effiency in high
performance mobile platforms, where there is a combination
of computationally expensive, latency tasks such as gaming
, while providing long battery for low processing intensity tasks such
as texting. They make the observation that
sequential parts of the application should be run
on powerful cores whilst parallel parts, which can leverage data
level parallelism or thread level parallelism, should be run
on multiple small cores. The chip has two types of cores
on it: the CortexA15 and the Coretex17. The Cortex7 is an in order dual issue
processor with a short pipeline (8 to 10 stages). By contrast,  the CortexA15
 is an out-of-order triple issue superscalar processor
which uses aggressive branch prediction and has a very deep pipeline
(between 15 and 24 stages).  When approparite, the Cortex A15 trades off energy
efficeicny for performance, while the Corte A7 trades off perfromance for energy efficiency.
Each can operate at a number of different oeprating points. This allows the OS 
to match the performance of the platform to the performance required by the application. 
There is a single cache coherent interconnect,  which 
facilitates full coherency between the two processors. 
This is important to note here, that it isn't the same tradeoff
as the i7 and the Tilera cores made, where the decision was between
a small number of powerful cores and a large number of simple cores. 
Here it's about given an application the performance it needs to function,
for the lowest possible budget, and this is achieved by a combination of 
using the simplest core possible for the job and operating points. 

\subsection{Identified tradeoffs}
TODO: need to make sure link back to core theme. 

The previous section gave an overview of the main types of multicore systems, 
and motivated their design choices based on their target applications. 
We expand on the major tradeoffs observed and discuss current research efforts in the area. 
\paragraph{Core Performance and Number of Cores}
\begin{itemize}
\item Powerful cores good for sequential execution but expensive
in terms of power. Focus on ILP. Wasteful in terms of power. 
\item More simple cores better because of (give mathematical reasoning)
\item Having multiple simple cores also allows you to better tailor
performance, using processor gating, voltage scaling, sleep mode. 
\item But not always: Ahdahmls' law, so bound by the sequential performance
\item Has some implications that introduces non-uniformity in the machines. 
\item Has some implications for the interconnect, now somehow needs to scale,
same for memory 
\item Also has the issue that puts the onus on the programmer to manage 
resources appropriately, and adapt to multicore. Has shown in 
the Analysis paper, not necessarity the case. Similaryl, 
also need to adapt programming model, so in seeking to maximise 
performance with respect to power, they are adding some complexity, 
and the onus is now on the programmer to adapt to this change. 
\item Additional consideration: not all applications require the fastest
possible performance. Operating points + lesser performing cores
are a good thing. 
\end{itemize}
\paragraph{Core Capabilities}

\paragraph{Instruction Set Compatibility }
\paragraph{Memory Coherency}
\paragraph{Specialisation vs Generality}

These various axis highlight varying degrees of heterogeneity both within
machines and across machines. Heterogeneity is a natural consequence of 
trying to maximise performance within a given power budget. It did not arise
out of choice but rather as the only possible solution for continuing to guarantee
good performance for a wide range of workload. The stringent requirements
on power fostered increased coupling between various hardware components.
X  highlights that, to come up with an optimal solution, all hardware components
should be codesigned. The shift from maximising performance to maximising performance
per watt increased specialisation of entire processors (for desktop, mobiles,
servers, etc.), but also of components within each processor. 

In the remainder of this paper, we survey how applications and operating
systems deal with internal and external heterogeneity in machines. 
In doing so, we observe two broad strategies: masking or exposing heterogeneity. 


\section{Multicore Architectures - Old}
\begin{itemize}
\item This section describes the three broad trends exhbited in designing
multicore systems. We compare and contrast the motivation
behind these design decisions and their impact in terms of performance/
programmability  
\begin{itemize}
\item Fewer, but more complex high performance cores with large,
shared on chip caches and cache coherence mechanisms
\item large number of simple, low frequency cores with distributed on chip
memories and scalable interconnects.
\item Integration of multiple cores of different capabilities and 
potentially supporting different instruction set architectures. 
\end{itemize}
\item For the first one, describe the Sandby Bridge Processor
\item Here the objective is to maximise performance for mostly sequential application
whilst exploiting a little bit of TLP when possible. 
\item For the second, describe Tilera architecture. Here, everything is about
low power cores, so power efficiency, but at the same time, they rely on 
homogeneous cores for programmbility. Optimised for highly parallel
applications, like DC. 
\begin{itemize}
\item Integrates large numbers of pwoer efficient processors interconencted by a scalable 
point ot point mesh network and minimise bot power and wire  delay issues
n\item Dynamic Distributed Cache. Fully coherent. 
\item Group things by tile. Each tile is a powerful, full featured computing systme 
that can independently run an os such as linux. 
An individaul tile is cpaable of executing upt o three ops per cycle. 
\item Processors use three way VLIW processor. Compile time scheduling of VLIW operations results in 
ower power consumption than dynamically scheduler superscalar processors. 
\item Mesh networks offier significantly lower latency and lower power than bus
or ring architectures, and because the bandwdith of mesh networks
scales as more tiles are added, can scale to thousands of tiles without 
becoming a bottleneck.

\end{itemize}
\item For the third, describe the BigLittle and the Cell processor. 
\begin{itemize}
\item Cell's objective was: acheive a large icnrease in performance, keeping the power
budget samll, whilst maintaining easy programmability. 
\item Combines a dual threaded dual issue 64 bit power architecture compliant
proecssor eleemnt PPE with 8 newly architectured synergistic proecssor elements, and an on chip
memory controller, and a controller for a convfigurable interface. 
\item Have a power and area efficent PPE that supports the ghih design fruency. SPEs for coherent offload. SPEs haev lcoal memroy,
asynchronous coherent DMA, and a large unified register filet oimrpove memeory bandwdithand to provide anew level of combined power
efficeicny and performance. 
\item SPE implements a new instruction set architecture optimised for 
power and performance. 
\item One of the big challenges is the existence of local store memory
and the fact that software must manage that memory. 
\end{itemize}
\begin{itemize}
\item Arm Little Big
\item the observation is that the sequential part of the application should 
run be run on powerful cores whilst parallel bits can be run on multiple msall ones. 
\item processor designed with goal of improving energy efficency in high
performance mobile platforms. 
\item make the bservation that require  platofs to be accompished at hgi proecsing
intensity tasks such as gaming and web browsing while providing long battery 
for low processing inetnsity tasks such as texting. 
\item key point is that though they provide different performance, they 
have the same architecture. 
\item When appropraite, Cortex A5 trades off energy efficiency for performanc
while
Cortex a8 trades off performance for energy efficiency. 
\item don't share a level 2 cache because what they are tyring to opmise is difeferent,
but they do share a single cache coehrent inteconnect/interrupt controller, which facilites tufll 
coherency between the proecssor. 
\item Different operating points to leveage dynamic voltage and frequency scaling. 
\item Cores which are not used are explicitly powered off.

\end{itemize}
\item Emphasise point that we are explicitly talking about hardware design here, 
not small optimisations or software. 

The conclusions that we can draw is this: 
\begin{itemize}
\item tradeoff between sequential and complex and parallel but simple cores. 
Also number of cores plays a part. Though more cores is good, you 
are limited by how much parallelism there is. 
Complex processors waste bandwdith due to 
speculatie techniques greated towards increasing the throughput
of each individual core. With less speculation, smaller cores can be more
area, power and badnwdtih efficient. This effiecny allows
a greater number of cores to be placed on chip and improves the overall
throughput of the chil ,e ven as the throughput of each core is lower compared to a more 
complex one. 
\item all cores have the same performance or not. Reasoning and implications.
Asymmetric chips can offer much better potential speedul
becuase they use the hgily powerful core
for the sequential parts of the code. On Chip heterogeneity allows the processor to better match
execution resoruces to each applciation's needs and to address a much 
wider of system laods with high efficiency.  Application
needs are not always so simply characterised and many types
of applciaions can benefit from either the speed of a large core or the efficncy of a msall core awt various points in their execution. 
Some application pahses might have a large amount of instruction level parallelism (ILP) whcih can be explocited bya core that can issue many instructions per cycle. The same core however might be very inefficeint for an applciaiton pahse with little ILP, consumping singifcanty more power than a simple core that is better matched to the application's characteristics. Therefore, in additional to changes in performance over time, sngificnat changes occur int he relative performance of the candidate core.  During serial portions  portions of execution, the chip's powerbudget is appleid toward using a single large core to allow the serila portion toe xecute as quickly as possible. 
During the parallel portions. the chips power is used more fficently bu runnign the parallel portion on a large number of small area and power efficeitn cores. Thus executing serial portions of an applciation byt relatively inefficient core ande xecution parallel portions of an algorithm on many small cores cna maximise the ratio 
of performance to pwoer dissipation. 
Pollack's rule: performance increase is roughly proportional to square root 
of increase in complexity. Applying Pollack's rule, 
performance of a msaller core reduces as square root of the size, but pwoer reduction is 
\item heterogeneous architecture or not. Demonstrate that having hetogeneous
(unconvential cores), can yield some huge performance guarantees, and 
even in the cases where off chip bandwdith is scarce. 
Custom logic and other low power U cores could potentially be used to
reduce sequential pwoer consumption or to efficiently 
imrpove sequential processing performance. First, if the goal is to ahchieve the same level of performance as a baseline system with proecssesos, a U Core can be used to speed up parallel sections of an application while allowing the sequetnail proecssor o slow down with a signifant reduction in power. Antoher perilsyp ropsoed method allwos a pwoer hugnry sequetnial rpecssor to offlado sectiosn of serial code to custom logic. 
But, difficult to program. The challenge is then to develop abstractions that makes it
not too hard to use them, like Dandelion for example or Hera JVM. 
\item memory coherent or not. 
\end{itemize}
\end{itemize}

\pagebreak 
