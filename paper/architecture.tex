
\section{Multicore Architectures}
\paragraph{}  This section surveys the major architectural decisions of modern multicore 
architectures. The high heterogeneity that multicore architectures exhibit
reflects the dual objectives of improving power effiency:
optimising performance within a given power budget on the one hand, 
and reducing power consumption whilst maintaining performance on the other.



\paragraph{} \emph{Inter-machine heterogeneity} can be explained by the
increased specialisation of  architectures for specific usecases. Chips for
mobile devices optimise for performance, reactivity, and power.
Processors for desktop optimise for cost-performance and graphics.
By contrast, servers prioritise throughput, scalability and energy,
whilst embedded devices optimise for energy and application-specific
performance. \emph {Intra-machine heterogeneity} can 
similarly be explained by the desire to optimise performance for power, or power
for performance. Each individual core is increasingly specialised
for a specific workload type. Different specialised cores then coexist within
a processor to support a wide range of applications. 

\paragraph{} We compare and contrast the motivation behind these
design decisions and their impact on the competing
concerns of power, performance, programmability.


\subsection{Architecture Overview}


Modern architectures cluster around three main categories:
1) designs with a small number of powerful cores 2) processors with large number of
simple, homogeneous cores 3) architectures which explicitly associate
cores of varying capabilities and instructions sets. We identify the
reasoning behind each design decision, focusing on a specific examlpe. 


\paragraph{Small number of complex high performance cores}
Various architectures place a small number (2-8) of highly
powerful cores on a chip.  Cores are mostly homogeneous, fully cache-coherent,
and generally own a private cache, but share the last level cache. 
Each core aggressively exploits instruction level parallelism, through
out-of-order execution, speculation, very deep pipelines, etc. 
These designs first and foremost optimise sequential performance, 
and leverage instruction level parallelism over thread level parallelism. 
This comes at the expense of high power consumption, 
(although most designs incorporate some power
saving techniques)
\paragraph{} The i7 Intel Core 920 contains 4 cores which aggresively
exploit ILP through a four issue,  dynamically scheduled 16 stage pipeline,
and multilevel branch prediction unit. All cores own
 private L1 and L2 caches, and share the L3 cache. They communicate
through a ring based interconnect. These architectures are
optimised for desktop computing, which has three distinguishing features:
applications are mostly designed with uni-processor in mind and are 
mostly latency sensitive. Secondly, these applications are built by 
programmers which do not necessary have in depth knowledge about computer
 architecture or operating systems. And finally, the performance requirements
of applications vary significantly, ranging from web-browsing
to graphics rendering.  This diversity requires cores to be very general,
so that they can adapt to the various kinds of workloads desktop computers
must support. Optimising for latency requires these cores to be very powerful 
and aggressively exploit ILP to guarantee good per
thread performance. The lack of knowledge about the underlying architecture
means that many developers continue to design applications with uniprocessors
in mind. This has two consequences, many applications have large sequential parts and 
little ILP. As performance is bound by the sequential parts of an application, 
complex cores are required to guarantee good performance. 
Adding more cores simply allows for new optimisations to be explored, namely, 
thread level parallelism, over the diminishing returns obtained by trying
to further exploit ILP.  Secondly, programmers often expect the 
sequential that they used on uniprocessors to be preserved. 
As a result, processors like the i7 put a lot of effort in exposing
a simple programming model, and hiding much of the underlying complexity, through
uniform cores, strong memory coherency, etc. 
The i7 distinguishes itself by its Turbo Boost technique (TODO: finish)


\paragraph{Large Number of Simple, Low Frequency Cores} An alternative
design strategy consists in placing a larger number of simpler, lower frequency cores
onto the chip. For the majority of architectures that follow these patterns, 
cores are homogeneous and are fully memory coherent. These architectures are
geared towards server workloads, that is, applications which  are mostly highly 
parallelisable, and throughput sensitive. They additionally require the ability
to scale whilst keeping power consumption low.   The design decisions therefore
 optimise throughput whilst minimising power consumption. 
They do so by exploiting a new kind of parallelism, thread-
level-parallelism (TLP). This allows for different parallel parts of the application
to be executed simultaneously on different cores. The looser latency 
requirements reduces the need for the - likely short - sequential
parts of the application to finish quickly. This in turn signifies that
there is no need to aggressively exploit ILP. Individual cores can thus be 
simpler, and run at a lower frequency. This reduces the static and dynamic
power consumption of each core, and allows cores to occupy a smaller
area, thus allowing to place more cores on a chip, which further increases throughput. 


\paragraph{} The Tilera architecture thus integrates a large number of low power, 
low frequency cores ~\cite{wentzlaff2007tile}. The TilePro64 processor contains 
64 cores, each operating at a frequency of 866 Mhz. This contrasts heavily
with the 4 cores of the i7, which each operated at a frequency of 2.66 Ghz (2.93 with
Turbo Boost)  Each tile is a compile-time scheduled triple-issue VLIW processor with 
a pipeline depth of only 5. Scheduling VLIW operations at compile time is 
significantly more power efficient
than relying on dynamic scheduling like most superscalar processors do (due to
static branch prediction and lack of out-of-order execution). Similarly,
reducing the pipeline depth to 5 (as opposed to 19 like the i7) reduces
power consumption. 
It is however more restrictive in the ILP that it can exploit. Cores are homogeneous and share 
an instruction set. This design decision was taken specifically to make it 
easier for applications to run on this architecture~\cite{wentzlaff2007tile}. Each
tile has a private cache. All share a fully coherent dynamic
distributed L3 cache.  Cache coherence is maintained
through each core having a home core, responsible for tracking
which cores share the cacheline. This directory based approach is significantly
more scalable than the traditional bus snooping technique.  The interconnect is a point to point mesh network, which
is highly scalable and minimises wire delays between cores. This
contrasts with the traditional ring based architecture, adopted for example
in the i7,  which, partly due to memory coherency protocols, quickly becomes a bottleneck
as the number of cores grows. This design allows for good performance
if the workload is highly parallelisable, but most
importantly, it provides the ability to scale whilst satisfying a low power budget. 
A Facebook study found that a task-parallel workload running on the TilePro64 exhibited
67\% higher throughput, whilst using 3 times less power than the corresponding
execution on an x86 based processor~\cite{berezecki2011manycore}. 

\paragraph{Heterogeneous Cores} The first category optimised for 
sequential performance and programmability at the expense of power. The second, 
for throughput, power and programmability at the expense of sequential performance. 
Core homogeneity, both in terms of instruction set and capability,
was key to facilitate application development by users. The last category
of multicore processors, by contrast, explicitly optimises both
power and performance, at the expense of programmibility. They do so in two ways: 
by associating cores with different capabilities on a chip, and possibly components
with different ISAs. This allows them to run tasks on the core
that is best suited for that workload: a mostly sequential taks will be run 
on a highly powerful core, whilst an easliy parraleisable workload
will be run on multiple simple cores. Similarly, certain chips incorporate
application-specific devices, such as GPUs or FPGAs which can achieve extremely
high performance in specific cases. Appropriately scheduling each 
task on the appropriate core complicates the task of the OS scheduler.
Programming for multiple ISAs increases the complexity of a developer's code. 

\paragraph{} The Cell Broadband Engine~\cite{kahle2005cell} illustrates the benefits obtainable with heterogeneous
cores with distinct ISAs. The processor is optimised for gaming, and had as primary 
requirements high performance for multimedia applications, and real-time
responsiveness to the user and the network, hence a combination of 
latency and throughput sensitive tasks. It had the additional goal 
to be applicable to a wide range of platforms, and hence provide 
both a programming model that is easy to adopt, and a sufficiently
general architecture. It combines a dual issue general power architecture 
compliant core (Power Processing Element), and 8 ``coprocessors''
cores (Synergistic Processing Elements).  The PPE is a dual issue core with a 
pipeline depth of 23. SPEs implement a new instruction set
architecture optmised for power and performance. They add an extra layer
to the memory hierarchy by each having some local memory, on which they storage instructions and data.  This is a technique
which hopes to alleviate the memory bandwidth wall, especially significant in
gaming applications. Both the PPE and the SPEs have
additional support for SIMD (Single Instruction, Multiple Data) instructions, through
which a large amount of data-level parallelism can be exploited, improving
power efficiency. The PPE has control over the SPEs and decides of task allocations,
 which processes to run, etc. Communication between cores is achieved 
through direct memory access (DMA) and synchronisation
mechanisms, via a high bandwidth bus (memory flow control). The processor itself is full 
custom, once again to maximise performance per watt. 

\paragraph{} The co-existence of two types of cores thus allows the Cell engine to match
the competing requirements of latency (by executing on the PPE),
and throughput, by offloading multimedia code to the SPEs. 
Programming on the Cell presents two challenges: firstly, the processor
combines two different kinds of instruction sets. Secondly, the existence of 
local store memory requires software to explicit manage it. 
The choice of having two different instruction sets can be explained 
by the desire to be able to run a standard OS one the main 
core whilst uploading computationally intensive/highly parallel work 
to the SPEs. Having a specialised architecture set optimised for multimedia
 and power increased power efficiency, whilst having a Power PC compliant 
architecture made it easier for developers to write programs on the Cell BE.  

\paragraph{} The Big.LITTLE processor~\cite{greenhalgh2011biglittle} strikes 
a different tradeoff, and expressly choses to have cores of different capabilities, but
with the same instruction set. The ARM Big.LITTLE is a processor
deisgned with the goal of improving power effiency in high
performance mobile platforms, where there is a combination
of computationally expensive, latency tasks such as gaming, 
while providing long battery for low processing intensity tasks such
as texting. Programmability is a major requirements as targeted developers
are not expected to understand the details of computer architecture. 
The develops of Big.LITTLE argue that sequential parts of the application 
should be run on powerful cores whilst parallel parts, which can leverage data
level parallelism or thread level parallelism, should be run
on multiple small cores. The chip has two types of cores
: the CortexA15 and the CortexA7. The CortexA7 is an in-order dual issue
processor with a short pipeline (8 to 10 stages). By contrast,  the CortexA15
 is an out-of-order triple issue superscalar processor
which uses aggressive branch prediction and has a very deep pipeline
(between 15 and 24 stages). Both share the same ISA. There is a single cache
coherent interconnect, which facilitiates full coherency between the 
two processors. Processes execute transparently on one core or the other.

\paragraph{} The key objective in Big.LITTLE is to optimise power effiency. It attempts to 
precisely match the core capabilities to performance requirements. The Cortex A15 trades off energy
efficiency for performance for latency sensitive applications. Performance critical applications
can benefit from a complex processor which aggressively exploits ILP. 
By contrast, the Cortex A7 trades off performance for energy efficiency. Low 
intensity applications are executed on the Cortex A7 to minimise their power 
consumption, simply ensuring that performance is ``good enough''.  The processor
additionally has the ability to operate at different operating points. When a tasks
exceeds the highest operating point of the Cortex A7, it is automatically migrated
to the more powerful core. 

\paragraph{} Whilst the Tile architecture and Big.Little both trade  simpler cores for 
more complex ones, their objective are different. Tilera designs optimise
for highly parallelisable applications and trade a small number of 
complex cores for a large number of simple ones. Big.Little does not
tradeoff core complexity for number of cores, but attempts to improve
the power efficiency of largely sequential applications, with differing
performance requirements,  by precisely tailoring the core performance to what
the application requires to function correctly. 

\subsection{Identified tradeoffs}

The previous section gave an overview of the main types of multicore systems, 
and motivated their design choices based on their target applications. 
We expand on the major tradeoffs observed and discuss current research efforts in the area. 
We focus specifically on inter-machine and intra-machine heterogeneity as a mechanism
to optimise power efficiency. 

\paragraph{Core Performance and Number of Cores}

Powerful and complex cores guarantee good sequential performance
through aggressively exploiting instruction level parallelism,
and high clock frequency. This requires complex hardware:
deeper pipelines, multi-level branch prediction units, etc.
This has implications both in terms of power consumption
and performance. Higher clock frequency allows for more 
instructions to be executed in a given period of time. This
improves latency, but increases dynamic power consumption as illustrated
in REF. Incorrect speculation also wastes bandwidth. CITE. 
Significantly, even when these components are not being used, 
static power consumption continues to increase, due to current leakage. 
static power consumption 
The relationship to performance is more complex. Complex cores
focus mostly on exploiting ILP, with limited opportunities
for exploiting Thread-Level-Parallelism and Data-Level-Parallelism
other than through Intel Hyperthreading CITE or SIMD extensions CITE. 
The diminishing returns observed by 

\begin{table*}
\begin{center}
\label{table:quantlaws}
\caption{Quantitative Laws of Computer Architecture}
\begin{tabular}{|c|c|}
Ahmdahl's Law CITE & $ Speedup \propto \frac{1}{(1 - P) + \frac{P}{N}} $\\ 
$Power_{static}$ CITE & $P_{static} = Current_{static} \times Voltage$ \\
$Power_{dynamic}$ CITE & $P_{dyn} = 1/2 \times Capacitive Load  \times Voltage^{2} \times Freq_{switched}$\\
Pollacks's Law CITE & $ Speedup \propto \sqrt{Complexity}$ and  $Power_{increase} \propto Complexity$\\
\end{tabular}
\end{center}

\end{table*}

We see that the optimal number of cores and their respective 
performance is heavily workload dependent. As processors
are increasingly specialised towards specific application
domains, the need for power and cost efficiency suggests
that there is going to be a large number of designs. 

\begin{itemize}
\item Powerful cores good for sequential execution but expensive
in terms of power. Focus on ILP. Wasteful in terms of power. 
Complex processors waste bandwdith due to 
speculatie techniques greated towards increasing the throughput
of each individual core. With less speculation, smaller cores can be more
area, power and badnwdtih efficient. This effiecny allows
a greater number of cores to be placed on chip and improves the overall
throughput of the chil ,e ven as the throughput of each core is lower compared to a more 
complex one. Pollack's rule: performance increase is roughly proportional to square root 
of increase in complexity. Applying Pollack's rule, 
performance of a msaller core reduces as square root of the size, but pwoer reduction is 
Simpler cores consume significantly less power as
a result of the combined effect of 1) less static power consumption because 
of fewer components on the chip 2) the ability to scale voltage down as
the clock rate is smaller.
\item More simple cores better because of (give mathematical reasoning)
\item Having multiple simple cores also allows you to better tailor
performance, using processor gating, voltage scaling, sleep mode. 
\item But not always: Ahdahmls' law, so bound by the sequential performance
\item Has some implications that introduces non-uniformity in the machines. 
\item Has some implications for the interconnect, now somehow needs to scale,
same for memory, currently become a bottleneck
\item Also has the issue that puts the onus on the programmer to manage 
resources appropriately, and adapt to multicore. Has shown in 
the Analysis paper, not necessarity the case. Similaryl, 
also need to adapt programming model, so in seeking to maximise 
performance with respect to power, they are adding some complexity, 
and the onus is now on the programmer to adapt to this change. 
\item Additional consideration: not all applications require the fastest
possible performance. Operating points + lesser performing cores
are a good thing. 
\end{itemize}
\paragraph{Core Capabilities}
\begin{itemize}
\item 
What we observe is two things: 
different kinds of workload perform best on different hardware,
not all workloads require the same performance characteristics,
good enough is good enough, most computers have to support a 
mix of workloads. 
\item The first makes a distinction between workloads
which have a lot of ILP to exploit, TLP, or DLP.
In the case of applications which have lots of ILP and
are performance sensitive, then they are best run on a fairly complex
processor. But for applications which alternatively
have a lot of TLP to use, then are better run
on simpler processors, because of the non linear 
relationasihp between completexity and performance, place more cores on chip, 
increase throuhput overall. 

Some application pahses might have a large amount of instruction level parallelism (ILP) whcih can be explocited bya core that can issue many instructions per cycle. The same core however might be very inefficeint for an applciaiton pahse with little ILP, consumping singifcanty more power than a simple core that is better matched to the application's characteristics. 
That means that you can either use simpler
cores to achieve the same performance, or achieve better performance whilst
keeping power constant. Wtih regards to DLP, then you have GPUS
(find memory equation). More generally, ``unconvential cores'' can
bring some significant memory savings. Demonstrate that having hetogeneous
(unconvential cores), can yield some huge performance guarantees, and 
even in the cases where off chip bandwdith is scarce. 
Custom logic and other low power U cores could potentially be used to
reduce sequential pwoer consumption or to efficiently 
imrpove sequential processing performance. First, if the goal is to ahchieve the same level of performance as a baseline system with proecssesos, a U Core can be used to speed up parallel sections of an application while allowing the sequetnail proecssor o slow down with a signifant reduction in power. Antoher perilsyp ropsoed method allwos a pwoer hugnry sequetnial rpecssor to offlado sectiosn of serial code to custom logic. 
\item The second makes the point that not all applications
are time critical, it is possible to run them on 
less powerful hardware for example, this allows you to trade
power for performance. This has some implications for the OS
which now needs to know how much is ``good enough'' for the 
task it is running. Examples include low intensity 
tasks in mobile phones. Other examlpes include the ability 
to run a key value store on wimpy nodes ~\cite{andersen2009fawn} (nodes powered
by a low power CPU), but this is not approaprite for all workloads~\cite{lang2010nonwimpy}.
Part of the reason there is because of network latency. 
\item The third observation is that most computers have to run 
a mix of workloads, tasks which are both throuhgput sensitive
and latency sensitive for example, tasks which are not time critical,
tasks which would perform best on a specific type of machine, etc. 
By adding these compoments to a chip, can precisely match the kind of 
core that you want. 
Therefore, in additional to changes in performance over time, sngificnat changes occur int he relative performance of the candidate core.  During serial portions  portions of execution, the chip's powerbudget is appleid toward using a single large core to allow the serila portion toe xecute as quickly as possible. 
During the parallel portions. the chips power is used more fficently bu runnign the parallel portion on a large number of small area and power efficeitn cores. Thus executing serial portions of an applciation byt relatively inefficient core ande xecution parallel portions of an algorithm on many small cores cna maximise the ratio 
of performance to pwoer dissipation. 

\item The implications however is that cores can now have different
capabilities, so not all tasks can be run on all cores, and as efficiently.
There's two challenges: find a way to express this core heterogeneity in
such a way that the OS or the application can understand it, and secondly
for the OS or the application to understand their workload sufficiently well
to know what to schedule where, and when (if the workload changes).  
\end{itemize}

\paragraph{Relationship between cores } As the number of cores increases,
and each core becomes more diverse, the question of the relationship
between cores becomes more important. 
\paragraph{Specialisation vs Generality} One of merging trends
is that multicore architectures are increasingly being
specialised for one specific purpose. As previously mentioned,
large, there are competing metrics, all which must meet certain
performance requirements, within a power budget. As a result,
the entire machine architecture is built to precisely 
fit purpose. The multicore area has significant increase
the number of dimensions along which computers can be
designed.The stringent requirements on power have 
 A specialised processor or chip can be more simple, by using more targeted
hardware. This can lead to power reduction because fewer unnecessary
compomenets are being used. This can also lead to performance
improvement because the capacity that's freed up by not using any  unnecessary
componennts can be used. 
 The stringent requirements
on power fostered increased coupling between various hardware components.
X  highlights that, to come up with an optimal solution, all hardware components
should be codesigned. The shift from maximising performance to maximising performance
per watt increased specialisation of entire processors (for desktop, mobiles,
servers, etc.), but also of components within each processor.
Given such high specialisation for specific domains, the question that we need
to ask, is how do we program for such devices, how can the software actually 
reflect that specialisation. 

\paragraph{}  These various axis highlight varying degrees of heterogeneity both within
machines and across machines. Heterogeneity is a natural consequence of 
trying to maximise performance within a given power budget. It did not arise
out of choice but rather as the only possible solution for continuing to guarantee good performance for a wide range of workloads. 

In the remainder of this paper, we survey how applications and operating
systems deal with internal and external heterogeneity in machines. 
In doing so, we observe two broad strategies: masking or exposing heterogeneity. 

