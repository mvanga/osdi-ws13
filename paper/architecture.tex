
\section{Multicore Architectures}
\paragraph{}  This section surveys the major architectural decisions of modern multicore 
architectures. The high heterogeneity that multicore architectures exhibit
reflects the dual objectives of optimising performance
for a given power budget on the one hand, and reducing power
consumption whilst keeping power constant, on the other. 

\paragraph{} \emph{Inter-machine heterogeneity} can be explained by the
increased specialisation of  architectures for specific usecases. Chips for
mobile devices optimise for performance, reactivity, and power.
Processors for desktop optimise for cost-performance and graphics.
By contrast, servers prioritise throughput, scalability and energy,
whilst embedded devices optimise for energy and application-specific
performance. \emph {Intra-machine heterogeneity} can 
similarly be explained by the desire to optimise performance for power, or power
for performance. Each individual core is increasingly specialised
for a specific workload type. Different specialised cores then coexist within
a processor to support a wide range of applications. 

\paragraph{} We compare and contrast the motivation behind these
design decisions and their impact on the competing
concerns of power, performance, programmability.


\subsection{Architecture Overview}


Modern architectures cluster around three main categories:
1) designs with a small number of powerful cores 2) processors with large number of
simple, homogeneous cores 3) architectures which explicitly associate
cores of varying capabilities and instructions sets. We identify the
reasoning behind each design decision, focusing on a specific examlpe. 


\paragraph{Small number of complex high performance cores}
Various architectures place a small number (2-8) of highly
powerful cores on a chip.  Cores are mostly homogeneous, fully cache-coherent,
and generally own a private cache, but share the last level cache. 
Each core aggressively exploits instruction level parallelism, through
out-of-order execution, speculation, very deep pipelines, etc. 
These designs first and foremost optimise sequential performance, 
and leverage instruction level parallelism over thread level parallelism. 
This comes at the expense of high power consumption, 
(although most designs incorporate some power
saving techniques)
\paragraph{} The i7 Intel Core 920 contains 4 cores which aggresively
exploit ILP through a four issue,  dynamically scheduled 16 stage pipeline,
and multilevel branch prediction unit. All cores own
 private L1 and L2 caches, and share the L3 cache. They communicate
through a ring based interconnect. These architectures are
optimised for desktop computing, which has three distinguishing features:
applications are mostly designed with uni-processor in mind and are 
mostly latency sensitive. Secondly, these applications are built by 
programmers which do not necessary have in depth knowledge about computer
 architecture or operating systems. And finally, the performance requirements
of applications vary significantly, ranging from web-browsing
to graphics rendering.  This diversity requires cores to be very general,
so that they can adapt to the various kinds of workloads desktop computers
must support. Optimising for latency requires these cores to be very powerful 
and aggressively exploit ILP to guarantee good per
thread performance. The lack of knowledge about the underlying architecture
means that many developers continue to design applications with uniprocessors
in mind. This has two consequences, many applications have large sequential parts and 
little ILP. As performance is bound by the sequential parts of an application, 
complex cores are required to guarantee good performance. 
Adding more cores simply allows for new optimisations to be explored, namely, 
thread level parallelism, over the diminishing returns obtained by trying
to further exploit ILP.  Secondly, programmers often expect the 
sequential that they used on uniprocessors to be preserved. 
As a result, processors like the i7 put a lot of effort in exposing
a simple programming model, and hiding much of the underlying complexity, through
uniform cores, strong memory coherency, etc. 
The i7 distinguishes itself by its Turbo Boost technique (TODO: finish)


\paragraph{Large Number of Simple, Low Frequency Cores} An alternative
design strategy consists in placing a larger number of simpler, lower frequency cores
onto the chip. For the majority of architectures that follow these patterns, 
cores are homogeneous and are fully memory coherent. These architectures are
geared towards server workloads, that is, applications which  are mostly highly 
parallelisable, and throughput sensitive. They additionally require the ability
to scale whilst keeping power consumption low.   The design decisions therefore
 optimise throughput whilst minimising power consumption. 
They do so by exploiting a new kind of parallelism, thread-
level-parallelism (TLP). This allows for different parallel parts of the application
to be executed simultaneously on different cores. The looser latency 
requirements reduces the need for the - likely short - sequential
parts of the application to finish quickly. This in turn signifies that
there is no need to aggressively exploit ILP. Individual cores can thus be 
simpler, and run at a lower frequency. This reduces the static and dynamic
power consumption of each core, and allows cores to occupy a smaller
area, thus allowing to place more cores on a chip, which further increases throughput. 


\paragraph{} The Tilera architecture thus integrates a large number of low power, 
low frequency cores ~\cite{wentzlaff2007tile}. The TilePro64 processor contains 
64 cores, each operating at a frequency of 866 Mhz. This contrasts heavily
with the 4 cores of the i7, which each operated at a frequency of 2.66 Ghz (2.93 with
Turbo Boost)  Each tile is a compile-time scheduled triple-issue VLIW processor with 
a pipeline depth of only 5. Scheduling VLIW operations at compile time is 
significantly more power efficient
than relying on dynamic scheduling like most superscalar processors do (due to
static branch prediction and lack of out-of-order execution). Similarly,
reducing the pipeline depth to 5 (as opposed to 19 like the i7) reduces
power consumption. 
It is however more restrictive in the ILP that it can exploit. Cores are homogeneous and share 
an instruction set. This design decision was taken specifically to make it 
easier for applications to run on this architecture~\cite{wentzlaff2007tile}. Each
tile has a private cache. All share a fully coherent dynamic
distributed L3 cache.  The interconnect is a point to point mesh network, which
is highly scalable and minimises wire delays between cores. This
contrasts with the traditional ring based architecture, adopted for example
in the i7,  which, partly due to memory coherency protocols, quickly becomes a bottleneck
as the number of cores grows. Cache coherence is maintained
through each core having a home core, responsible for tracking
which cores share the cacheline. This directory based approach is significantly
more scalable than the traditional bus snooping technique.  This design allows for good performance
if the workload is highly parallelisable, but most
importantly, it provides the ability to scale whilst satisfying a low power budget. 
A Facebook study found that a task-parallel workload running on the TilePro64 exhibited
67\% higher throughput, whilst using 3 times less power than the corresponding
execution on an x86 based processor~\cite{berezecki2011manycore}. 

\paragraph{Heterogeneous Cores} The first category optimised for 
sequential performance and programmability at the expense of power. The second, 
for throughput, power and programmability at the expense of sequential performance. 
Core homogeneity, both in terms of instruction set and capability,
was key to facilitate application development by users. The last category
of multicore processors, by contrast, explicitly optimises both
power and performance, at the expense of programmibility. They do so in two ways: 
by associating cores with different capabilities on a chip, and possibly components
with different ISAs. This allows them to run tasks on the core
that is best suited for that workload: a mostly sequential taks will be run 
on a highly powerful core, whilst an easliy parraleisable workload
will be run on multiple simple cores. Similarly, certain chips incorporate
application-specific devices, such as GPUs or FPGAs which can achieve extremely
high performance in specific cases. Appropriately scheduling each 
task on the appropriate core complicates the task of the OS scheduler.
Programming for multiple ISAs increases the complexity of a developer's code. 

\paragraph{}The Cell BE illustrates the benefits obtainable with heterogeneous
cores with distinct ISAs. The processor combines a 
dual issue general power architecture compliant core (Power Processing Element), and 8 ``coprocessors''
cores (Synergistic Processing Elements). These implement a new instruction sete
architecture optmised for power and performance.  The PPE has control over the SPE
and decides of task allocations, which processes to run, etc. SPEs can only 
access their own local memory, and rely on DMA for all other forms of computation.
The system impleents a fully cache coherent DMA to access both memory and disk. 
The PPE and all the SPEs communicate via a high bandwdith bus. 
Programming on the Cell presents two challenges: firstly, the different kinds of 
instruction sets present. And secondly, the existence of local store memory and the
fact that software must manage that memory. The choice of having two different instruction
sets can be explained by the desire to be able to run a standard OS one the main 
core whilst uploading computationally intensive/highly parallel work 
to the SPEs. Having a specialised architecture set optimised for power was
because of the stated objective: icnrease a large icnrease in performance,
keeping the pwoer budget small.  It's worth to note though that the 
fully coherent DMA does make things easier.  Cell BE is used in 
many game consoles. A similar design decision was taken in the Nvida Tigra 2, which is 
a processor geared towards mobile systems. It integrates a dual
core ARM Cortex A9 and GPU on a single chip using a single physical memory. 

\paragraph{} The BigLittle processor struck a different tradeoff,
and expressly chose to have cores of different capabilities, but
with the same instruction set. The Arm LittleBig is a processor
deisgned with the goal of improving power effiency in high
performance mobile platforms, where there is a combination
of computationally expensive, latency tasks such as gaming
, while providing long battery for low processing intensity tasks such
as texting. They make the observation that
sequential parts of the application should be run
on powerful cores whilst parallel parts, which can leverage data
level parallelism or thread level parallelism, should be run
on multiple small cores. The chip has two types of cores
on it: the CortexA15 and the Coretex17. The Cortex7 is an in order dual issue
processor with a short pipeline (8 to 10 stages). By contrast,  the CortexA15
 is an out-of-order triple issue superscalar processor
which uses aggressive branch prediction and has a very deep pipeline
(between 15 and 24 stages).  When approparite, the Cortex A15 trades off energy
efficeicny for performance, while the Corte A7 trades off perfromance for energy efficiency.
Each can operate at a number of different oeprating points. This allows the OS 
to match the performance of the platform to the performance required by the application. 
There is a single cache coherent interconnect,  which 
facilitates full coherency between the two processors. 
This is important to note here, that it isn't the same tradeoff
as the i7 and the Tilera cores made, where the decision was between
a small number of powerful cores and a large number of simple cores. 
Here it's about given an application the performance it needs to function,
for the lowest possible budget, and this is achieved by a combination of 
using the simplest core possible for the job and operating points. 

\subsection{Identified tradeoffs}
TODO: need to make sure link back to core theme. 

The previous section gave an overview of the main types of multicore systems, 
and motivated their design choices based on their target applications. 
We expand on the major tradeoffs observed and discuss current research efforts in the area. 
All of these introduce and characterise the internal and external heterogeneity that 
modern multicores exhibit. 
One thing to note is that you can make tradeoffs with different objectives:
you can try to keep the same power budget whilst improving performance. Or
you can try to keep performance constant whilst reducing power consumption. 
How far you are willing to go has

\paragraph{Core Performance and Number of Cores}
\begin{itemize}
\item Powerful cores good for sequential execution but expensive
in terms of power. Focus on ILP. Wasteful in terms of power. 
Complex processors waste bandwdith due to 
speculatie techniques greated towards increasing the throughput
of each individual core. With less speculation, smaller cores can be more
area, power and badnwdtih efficient. This effiecny allows
a greater number of cores to be placed on chip and improves the overall
throughput of the chil ,e ven as the throughput of each core is lower compared to a more 
complex one. Pollack's rule: performance increase is roughly proportional to square root 
of increase in complexity. Applying Pollack's rule, 
performance of a msaller core reduces as square root of the size, but pwoer reduction is 
Simpler cores consume significantly less power as
a result of the combined effect of 1) less static power consumption because 
of fewer components on the chip 2) the ability to scale voltage down as
the clock rate is smaller.
\item More simple cores better because of (give mathematical reasoning)
\item Having multiple simple cores also allows you to better tailor
performance, using processor gating, voltage scaling, sleep mode. 
\item But not always: Ahdahmls' law, so bound by the sequential performance
\item Has some implications that introduces non-uniformity in the machines. 
\item Has some implications for the interconnect, now somehow needs to scale,
same for memory, currently become a bottleneck
\item Also has the issue that puts the onus on the programmer to manage 
resources appropriately, and adapt to multicore. Has shown in 
the Analysis paper, not necessarity the case. Similaryl, 
also need to adapt programming model, so in seeking to maximise 
performance with respect to power, they are adding some complexity, 
and the onus is now on the programmer to adapt to this change. 
\item Additional consideration: not all applications require the fastest
possible performance. Operating points + lesser performing cores
are a good thing. 
\end{itemize}
\paragraph{Core Capabilities}
\begin{itemize}
\item 
What we observe is two things: 
different kinds of workload perform best on different hardware,
not all workloads require the same performance characteristics,
good enough is good enough, most computers have to support a 
mix of workloads. 
\item The first makes a distinction between workloads
which have a lot of ILP to exploit, TLP, or DLP.
In the case of applications which have lots of ILP and
are performance sensitive, then they are best run on a fairly complex
processor. But for applications which alternatively
have a lot of TLP to use, then are better run
on simpler processors, because of the non linear 
relationasihp between completexity and performance, place more cores on chip, 
increase throuhput overall. 

Some application pahses might have a large amount of instruction level parallelism (ILP) whcih can be explocited bya core that can issue many instructions per cycle. The same core however might be very inefficeint for an applciaiton pahse with little ILP, consumping singifcanty more power than a simple core that is better matched to the application's characteristics. 
That means that you can either use simpler
cores to achieve the same performance, or achieve better performance whilst
keeping power constant. Wtih regards to DLP, then you have GPUS
(find memory equation). More generally, ``unconvential cores'' can
bring some significant memory savings. Demonstrate that having hetogeneous
(unconvential cores), can yield some huge performance guarantees, and 
even in the cases where off chip bandwdith is scarce. 
Custom logic and other low power U cores could potentially be used to
reduce sequential pwoer consumption or to efficiently 
imrpove sequential processing performance. First, if the goal is to ahchieve the same level of performance as a baseline system with proecssesos, a U Core can be used to speed up parallel sections of an application while allowing the sequetnail proecssor o slow down with a signifant reduction in power. Antoher perilsyp ropsoed method allwos a pwoer hugnry sequetnial rpecssor to offlado sectiosn of serial code to custom logic. 
\item The second makes the point that not all applications
are time critical, it is possible to run them on 
less powerful hardware for example, this allows you to trade
power for performance. This has some implications for the OS
which now needs to know how much is ``good enough'' for the 
task it is running. Examples include low intensity 
tasks in mobile phones. Other examlpes include the ability 
to run a key value store on wimpy nodes ~\cite{andersen2009fawn} (nodes powered
by a low power CPU), but this is not approaprite for all workloads~\cite{lang2010nonwimpy}.
Part of the reason there is because of network latency. 
\item The third observation is that most computers have to run 
a mix of workloads, tasks which are both throuhgput sensitive
and latency sensitive for example, tasks which are not time critical,
tasks which would perform best on a specific type of machine, etc. 
By adding these compoments to a chip, can precisely match the kind of 
core that you want. 
Therefore, in additional to changes in performance over time, sngificnat changes occur int he relative performance of the candidate core.  During serial portions  portions of execution, the chip's powerbudget is appleid toward using a single large core to allow the serila portion toe xecute as quickly as possible. 
During the parallel portions. the chips power is used more fficently bu runnign the parallel portion on a large number of small area and power efficeitn cores. Thus executing serial portions of an applciation byt relatively inefficient core ande xecution parallel portions of an algorithm on many small cores cna maximise the ratio 
of performance to pwoer dissipation. 

\item The implications however is that cores can now have different
capabilities, so not all tasks can be run on all cores, and as efficiently.
There's two challenges: find a way to express this core heterogeneity in
such a way that the OS or the application can understand it, and secondly
for the OS or the application to understand their workload sufficiently well
to know what to schedule where, and when (if the workload changes).  
\end{itemize}

\paragraph{Relationship between cores } As the number of cores increases,
and each core becomes more diverse, the question of the relationship
between cores becomes more important. 
\paragraph{Specialisation vs Generality} One of merging trends
is that multicore architectures are increasingly being
specialised for one specific purpose. As previously mentioned,
large, there are competing metrics, all which must meet certain
performance requirements, within a power budget. As a result,
the entire machine architecture is built to precisely 
fit purpose. The multicore area has significant increase
the number of dimensions along which computers can be
designed.The stringent requirements on power have 
 A specialised processor or chip can be more simple, by using more targeted
hardware. This can lead to power reduction because fewer unnecessary
compomenets are being used. This can also lead to performance
improvement because the capacity that's freed up by not using any  unnecessary
componennts can be used. 
 The stringent requirements
on power fostered increased coupling between various hardware components.
X  highlights that, to come up with an optimal solution, all hardware components
should be codesigned. The shift from maximising performance to maximising performance
per watt increased specialisation of entire processors (for desktop, mobiles,
servers, etc.), but also of components within each processor.
Given such high specialisation for specific domains, the question that we need
to ask, is how do we program for such devices, how can the software actually 
reflect that specialisation. 

\paragraph{}  These various axis highlight varying degrees of heterogeneity both within
machines and across machines. Heterogeneity is a natural consequence of 
trying to maximise performance within a given power budget. It did not arise
out of choice but rather as the only possible solution for continuing to guarantee good performance for a wide range of workloads. 

In the remainder of this paper, we survey how applications and operating
systems deal with internal and external heterogeneity in machines. 
In doing so, we observe two broad strategies: masking or exposing heterogeneity. 

